{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdcd28c3-ead6-4611-9eb3-ebf16948a805",
   "metadata": {},
   "source": [
    "### Chatbot_QA_Data_Augmentation_and_Evaluation\n",
    "\n",
    "This notebook contains the following sections:\n",
    "1. Import the dataset and then use LLM to generate question-answer example based on context from training data/RAG-chain\n",
    "2. LLM assisted evaluation on Generated Question and Predicted Answer\n",
    "3. Manual Evaluation / Review\n",
    "4. Human Correction\n",
    "5. Add generated qa pairs into dataset\n",
    "6. Save the new dataset as a jsonl file for next round's evaluation use\n",
    "7. Test loading the new dataset to make sure it works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b9496e-b67e-4ff5-bd07-f0b887836e12",
   "metadata": {},
   "source": [
    "### Import the dataset and then use LLM to generate question-answer example based on context from training data/RAG-chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb442c-21fd-4e5e-b4e0-bc4d919e9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
    "\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "import langchain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain.evaluation.qa import ContextQAEvalChain\n",
    "\n",
    "from langchain.schema import Document\n",
    "import copy\n",
    "\n",
    "import json\n",
    "from typing import Iterable\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None) # Set pandas display options to show all characters\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning) # Suppress only UserWarnings\n",
    "warnings.filterwarnings(\"ignore\") # Suppress all Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b515fb2-f749-4d50-a6c6-a406c0c0d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "dataset_name = \"MakTek/Customer_support_faqs_dataset\"\n",
    "page_content_column = \"answer\"\n",
    "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e9bd7d-536a-4f51-81e3-1f9e0b8baa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine batch size\n",
    "\n",
    "batch_size = len(data)\n",
    "# batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b990cf-ffe5-4e2f-9055-6c46954f507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model and prompt used in the QA-generation chain\n",
    "\n",
    "custom_qa_gen_prompt_str = \"\"\"System: You are the manager of Customer Service Support coming up with example questions from customers.\n",
    "Given the following document, please generate a question and answer based on that document.\n",
    "\n",
    "Example Format:\n",
    "<Begin Document>\n",
    "...\n",
    "<End Document>\n",
    "QUESTION: question here\n",
    "ANSWER: answer here\n",
    "\n",
    "Instruction: These questions should be detailed and be based explicitly on information in the document. Begin!\n",
    "\n",
    "<Begin Document>\n",
    "{doc}\n",
    "<End Document>\"\"\"\n",
    "\n",
    "custom_qa_gen_prompt = PromptTemplate(input_variable=[\"doc\"], template=custom_qa_gen_prompt_str)\n",
    "\n",
    "langchain.evaluation.qa.generate_prompt.PROMPT = custom_qa_gen_prompt\n",
    "\n",
    "llm_model = \"llama3.2\"\n",
    "example_gen_chain = QAGenerateChain.from_llm(OllamaLLM(model=llm_model))\n",
    "\n",
    "langchain.evaluation.qa.generate_prompt.PROMPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0530104b-d993-45a0-b6c3-da28572cc531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated new qa examples\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "new_examples = []\n",
    "for item in tqdm([{\"doc\": t} for t in data[:batch_size]], desc=\"Generating QA Examples\"):\n",
    "    result = example_gen_chain.apply_and_parse([item])\n",
    "    new_examples.append(result)\n",
    "\n",
    "# A trick to flaten the list:\n",
    "new_examples = sum(new_examples, [])\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time) / 60\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349f8ac-8860-4a1a-a456-57327c76cfaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create dataframe for the generated_qa_examples\n",
    "\n",
    "generated_qa_examples = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    generated_qa_examples.append({\"Dataset Index\" : str(i),\n",
    "                                  \"Train-set Question\" : data[i].metadata['question'],\n",
    "                                  \"Generated Question\" : new_examples[i]['qa_pairs']['query'],\n",
    "                                  \"Train-set Context\" : data[i].page_content,\n",
    "                                  \"Generated Answer\" : new_examples[i]['qa_pairs']['answer']\n",
    "                                 })\n",
    "\n",
    "generated_qa_examples_df = pd.DataFrame(generated_qa_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab4a915-17e7-42fd-a33e-0f13d96519dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_qa_examples_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ba4c1e-a70b-411d-bfaf-c45c30d9f3bf",
   "metadata": {},
   "source": [
    "### LLM assisted evaluation on Generated Question and Predicted Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e6627-23da-4066-9e75-204218e13e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's create a chain.\n",
    "# Let's use the same kind of chain we used in our customer service chatbot for consistency.\n",
    "# However, for this evaluation, we don't need to use the memory for simplicity.\n",
    "\n",
    "embedding_model_chosen = \"hkunlp/instructor-large\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=embedding_model_chosen,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs)\n",
    "\n",
    "persist_directory = 'docs/chroma/'\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=data,\n",
    "    embedding=hf_embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "retriever=vectordb.as_retriever(search_kwargs={\"k\": 3}, search_type=\"mmr\")\n",
    "\n",
    "base_prompt_template = \"\"\"System: You are a ABC-Company customer service representative.\n",
    "\\n\\nInstruction: First, if you know the answer: Answer the customer's question based on following context and chat history. Do not mention we have discussed this topic before in the previous conversation or ask any follow up question. Otherwise, if you do not know the answer: simply answer 'I am not sure about the answer, please contact our human service for assistance. Thank You!'.\n",
    "\\n\\nContext: {context}\n",
    "\\n\\nChat history: {chat_history}\n",
    "\\n\\nQuestion: {question}\n",
    "\\n\\nOutput Answer: \"\"\"\n",
    "prompt_input_list = [\"context\", \"question\", \"chat_history\"]\n",
    "\n",
    "BASE_PROMPT = PromptTemplate(\n",
    "            template=base_prompt_template, input_variables=[\"context\", \"question\", \"chat_history\"]\n",
    "        )\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key='question',\n",
    "    output_key='answer',\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=OllamaLLM(model=llm_model, temperature = 0.01),\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    #return_generated_question=True,\n",
    "    verbose=False,\n",
    "    combine_docs_chain_kwargs={'prompt': BASE_PROMPT},\n",
    "    # condense_question_prompt=condense_question_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6cac5e-48f8-4802-be00-6d2eba109a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure memory is clear\n",
    "\n",
    "memory.clear()\n",
    "memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193018c3-42c2-40ae-8887-ffda80dafabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the format for the generated question set\n",
    "\n",
    "prediction_question_list = []\n",
    "for i in range(batch_size):\n",
    "    prediction_question_list.append(new_examples[i]['qa_pairs']['query'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869e829c-6619-494e-880f-27f59a1cc631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the generated question into the llm, generate answer and save the context and predicted answer\n",
    "start_time = time.time()\n",
    "\n",
    "prediction_answer_list = []\n",
    "for question in tqdm(prediction_question_list, desc=\"Predicting answers for generated questions\"):\n",
    "    memory.clear() # This is necessary because otherwise the memory will becomes too long and response will be too slow\n",
    "    answer = qa(question)  # Generate an answer\n",
    "\n",
    "    #append question, answer and source_context\n",
    "    if answer['source_documents'][0].page_content == answer['source_documents'][1].page_content == answer['source_documents'][2].page_content:\n",
    "        prediction_answer_list.append({\"question\": question,\n",
    "                                        \"answer\": answer['answer'],\n",
    "                                        \"source_context\": answer['source_documents'][0].page_content})        \n",
    "    elif (answer['source_documents'][0].page_content == answer['source_documents'][1].page_content) or (answer['source_documents'][1].page_content == answer['source_documents'][2].page_content):\n",
    "        prediction_answer_list.append({\"question\": question,\n",
    "                                        \"answer\": answer['answer'],\n",
    "                                        \"source_context\": answer['source_documents'][0].page_content + answer['source_documents'][2].page_content})\n",
    "    else:\n",
    "        prediction_answer_list.append({\"question\": question,\n",
    "                                        \"answer\": answer['answer'],\n",
    "                                        \"source_context\": answer['source_documents'][0].page_content + answer['source_documents'][1].page_content + answer['source_documents'][2].page_content})\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time) / 60\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a66365c-b16d-421b-8b46-a5f1fe2ae83b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create data frame for prediction_answer_list\n",
    "\n",
    "prediction_answer_list_df = pd.DataFrame(prediction_answer_list)\n",
    "prediction_answer_list_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899e085-6ffb-44d8-a689-314b6f15fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new grading prompt to edit the roles in the evaluation chain\n",
    "\n",
    "custom_grading_prompt_str = \"\"\"System: You are a Customer Service Manager grading a Customer Service Chatbot's responses to customers' questions.\n",
    "You are given a question, the context the question is about, and the Customer Service Chatbot's answer. You are asked to score the Chatbot's answer as either CORRECT or INCORRECT, based on the context.\n",
    "\n",
    "Instruction: Grade the chatbot answer INCORRECT if it makes up more details than the provided context. Grade the chatbot answer as CORRECT even if the chatbot answer misses some details and does not mention all the context information provided. Ignore differences in punctuation and phrasing between the chatbot answer and context. \n",
    "\n",
    "Example Format:\n",
    "CUSTOMER QUESTION: customer question here\n",
    "CONTEXT: context the question is about here\n",
    "CUSTOMER SERVICE CHATBOT ANSWER: customer service chatbot's answer here\n",
    "GRADE: CORRECT or INCORRECT here\n",
    "\n",
    "CUSTOMER QUESTION: {query}\n",
    "CONTEXT: {context}\n",
    "CUSTOMER SERVICE CHATBOT ANSWER: {result}\n",
    "GRADE: \"\"\"\n",
    "\n",
    "custom_grading_prompt = PromptTemplate(input_variable=[\"query\", \"context\", \"result\"], template=custom_grading_prompt_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdaf15b-d034-4d80-a269-695766c59cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.evaluation.qa.eval_prompt.CONTEXT_PROMPT = custom_grading_prompt\n",
    "langchain.evaluation.qa.eval_prompt.PROMPT = custom_grading_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e485eac7-2bb5-473e-8017-38ba3dff2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the prompt used in the context evaluation chain\n",
    "# langchain.evaluation.qa.eval_prompt.CONTEXT_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718f809e-df16-4b64-afa8-80d5490471ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the prompt used in the context evaluation chain\n",
    "# langchain.evaluation.qa.eval_prompt.PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a3504-2548-4553-a923-ae6c8142caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OllamaLLM(model=llm_model)\n",
    "# eval_chain = QAEvalChain.from_llm(llm)\n",
    "eval_chain = ContextQAEvalChain.from_llm(llm, prompt = custom_grading_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf300e8-f050-4997-a1cd-48fa95dfbc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the answer\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"...Start Evaluating Answers...\")\n",
    "graded_outputs = eval_chain.evaluate(examples=prediction_answer_list, \n",
    "                                     predictions=prediction_answer_list, \n",
    "                                     question_key=\"question\", \n",
    "                                     context_key=\"source_context\", \n",
    "                                     prediction_key=\"answer\")\n",
    "print(\"...Done Evaluating Answers...\")\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time) / 60\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46294310-c598-4644-9d16-db31e626bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_incorrect_counter = 0\n",
    "total_correct_counter = 0\n",
    "human_incorrect_ans_review = []\n",
    "human_correct_ans_review = []\n",
    "\n",
    "for i, eg in enumerate(prediction_answer_list):\n",
    "    \n",
    "    if \"INCORRECT\" in graded_outputs[i]['text']:\n",
    "        total_incorrect_counter += 1\n",
    "        append_list = human_incorrect_ans_review\n",
    "        \n",
    "    elif \"CORRECT\" in graded_outputs[i]['text']:\n",
    "        total_correct_counter += 1\n",
    "        append_list = human_correct_ans_review\n",
    "\n",
    "    append_list.append({\"Generate-Index\" : i,\n",
    "                             \"Generated Question\" :prediction_answer_list[i]['question'],\n",
    "                              \"Context Answer\" :prediction_answer_list[i]['source_context'],\n",
    "                              \"Generated Answer\" :prediction_answer_list[i]['answer'],\n",
    "                              \"Predicted Grade\" :graded_outputs[i]['text']}\n",
    "                                       )\n",
    "\n",
    "human_incorrect_ans_review_df = pd.DataFrame(human_incorrect_ans_review)\n",
    "human_correct_ans_review_df = pd.DataFrame(human_correct_ans_review)\n",
    "\n",
    "print(\"Machine Evaluation Total Score: \" + str(int(total_correct_counter*100/len(graded_outputs))) + \"%\")\n",
    "print(\"Machine Evaulation Total Incorrect Ans: \" + str(int(total_incorrect_counter*100/len(graded_outputs))) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389a7d79-85de-4733-9555-eedc8ebde2d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "human_incorrect_ans_review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ae0e7-3195-49ac-bd26-cf89725e37b4",
   "metadata": {},
   "source": [
    "### Manual Evaluation / Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab0f49-126b-4150-9813-840c02e73e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain.debug = True\n",
    "# memory.clear()\n",
    "# qa.invoke(new_examples[2]['qa_pairs'][\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94373644-517d-46e3-bd3c-1e583c214d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3072611f-231e-4ef9-9db4-ee14268dc599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the human_incorrect_ans_review df to those the chatbot said he did not know the answer\n",
    "# We will add those answers to the data to expand and improve the chatbot's RAG knowledge capacity\n",
    "\n",
    "dunno_ans_df = human_incorrect_ans_review_df[human_incorrect_ans_review_df['Generated Answer'].str.contains(\"I am not sure about the answer\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d7bfc-4865-4a68-ba02-101cc5572843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dunno_ans_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd7230-225d-4eb4-8b8a-10dae2641ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the human_incorrect_ans_review df to those the chatbot provided maybe truly incorrect answer\n",
    "# We will review those manually. \n",
    "# If it is actually correct, we will move it to the correct list\n",
    "# If it is actually incorrect, we will add the data to expand and improve the chatbot's RAG knowledge capacity\n",
    "\n",
    "human_filtered_incorrect_ans_review_df = human_incorrect_ans_review_df[~human_incorrect_ans_review_df['Generate-Index'].isin(dunno_ans_df['Generate-Index'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96c439-a5c3-4731-aa84-6b5504d8ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_filtered_incorrect_ans_review_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82521f85-9b39-42a5-af8b-bbb0c3c414bd",
   "metadata": {},
   "source": [
    "### Human Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c59785-1dff-4562-ab4e-92c7eb000994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enter the Generate-Index of the questions you want to move into the correct list and those you want to keep in the incorrect list\n",
    "human_correct_list_generate_index = [17, 27, 46, 63, 76, 85, 96, 117, 123, 127, 146, 173, 187]\n",
    "\n",
    "# Create a df and change the Predicted Grade to CORRECT\n",
    "human_correct_df = human_filtered_incorrect_ans_review_df[human_filtered_incorrect_ans_review_df['Generate-Index'].isin(human_correct_list_generate_index)]\n",
    "human_correct_df[\"Predicted Grade\"] = \"HUMAN REVIEWED GRADE: CORRECT\"\n",
    "\n",
    "human_correct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d54fdd-5a15-4b88-afd7-e03d79367019",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_correct_ans_final_df = pd.concat([human_correct_ans_review_df, human_correct_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d51d65c-14c5-4a64-817b-540085ea0474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "human_correct_ans_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17be4da-8d33-469d-83fe-ed07979f0fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(human_correct_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a22c70-671b-43d6-a338-0d8917922b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Chatbot performance score again after human review\n",
    "\n",
    "reviewed_correct_counter = total_correct_counter + len(human_correct_df)\n",
    "reviewed_incorrect_counter = total_incorrect_counter - len(human_correct_df)\n",
    "\n",
    "print(\"Human Reviewed Evaluation Total Score: \" + str(int(reviewed_correct_counter*100/len(graded_outputs))) + \"%\")\n",
    "print(\"Human Reviewed Evaulation Total Incorrect Ans: \" + str(int(reviewed_incorrect_counter*100/len(graded_outputs))) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e24b5b5-bc22-478a-9f07-c8c05c66ec01",
   "metadata": {},
   "source": [
    "### Add generated qa pairs into dataset\n",
    "- After human reviewing the evaluated incorrect answers, to improve the llm chatbot, we will add the selected generated qa pairs into the dataset for the next round of evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43940113-7224-4384-a2d8-0880112ed518",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_qa_examples_df[\"Dataset Index\"].dtype)\n",
    "print(human_correct_ans_final_df[\"Generate-Index\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5276a4c2-d850-4662-8265-1219b0f460b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_qa_examples_df[\"Dataset Index\"] = generated_qa_examples_df[\"Dataset Index\"].astype(int)\n",
    "human_correct_ans_final_df[\"Generate-Index\"] = human_correct_ans_final_df[\"Generate-Index\"].astype(int)\n",
    "\n",
    "data_to_add_df = generated_qa_examples_df[~generated_qa_examples_df[\"Dataset Index\"].isin(human_correct_ans_final_df[\"Generate-Index\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58125ac5-ed38-4b9e-ab83-6cf90c58f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_add_df\n",
    "data_to_add_df_no_duplicate = data_to_add_df.drop_duplicates()\n",
    "data_to_add_df_no_duplicate.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c0a7e-723d-4f8a-9c7f-fd64f0eb8f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_deep_copy = copy.deepcopy(data)\n",
    "# data_deep_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f72b4e2-db2d-473d-bbc4-ec903caeac01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(data_to_add_df_no_duplicate)):\n",
    "    data_to_append = Document(metadata={'question': data_to_add_df_no_duplicate[\"Generated Question\"][i]},\n",
    "                       # page_content=data_to_add_df_no_duplicate[\"Generated Answer\"][i]\n",
    "                      page_content = \"Example Question: \" + data_to_add_df_no_duplicate[\"Generated Question\"][i] + \" \" + \"Example Answer: \" + data_to_add_df_no_duplicate[\"Generated Answer\"][i]\n",
    "                             )\n",
    "    data_deep_copy.append(data_to_append)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bd025-14d4-4478-b72b-2f92857da57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "print(len(data_to_add_df_no_duplicate))\n",
    "print(len(data_deep_copy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e4bcbf-8bd8-4151-a60c-72fa2e208392",
   "metadata": {},
   "source": [
    "### Save the new dataset as a jsonl file for next round's evaluation use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247faace-6798-4938-8f9a-f12547595af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_docs_to_jsonl(array:Iterable[Document], file_path:str)->None:\n",
    "    with open(file_path, 'w') as jsonl_file:\n",
    "        for doc in array:\n",
    "            jsonl_file.write(doc.json() + '\\n')\n",
    "\n",
    "def load_docs_from_jsonl(file_path)->Iterable[Document]:\n",
    "    array = []\n",
    "    with open(file_path, 'r') as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            data = json.loads(line)\n",
    "            obj = Document(**data)\n",
    "            array.append(obj)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d43c1b-565b-4002-a425-6c42d0fa9f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_docs_to_jsonl(data_deep_copy,'dataset_with_appended_new_qa.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0760a6-e009-45fd-8a6a-51809fe4ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_to_add_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec74f718-31b2-4011-a443-8d58229ad519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "# data_to_add_df.to_csv(\"data_to_add_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de93d56-0eb5-4ab5-930b-a293c2a3965c",
   "metadata": {},
   "source": [
    "### Test loading the new dataset to make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eab0b3-3656-444b-b740-7604563d8175",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_appended_new_qa=load_docs_from_jsonl('dataset_with_appended_new_qa.jsonl')\n",
    "print(len(dataset_with_appended_new_qa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad4ff7a-e874-4633-96dd-5a554fb37b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_appended_new_qa[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf268e-87c2-4de4-a16c-d36d426d955f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
